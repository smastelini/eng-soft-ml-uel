{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3\n",
    "\n",
    "---\n",
    "\n",
    "**Saulo Martiello Mastelini**<br>\n",
    "Candidato a Ph.D. pelo ICMC-USP<br>\n",
    "Online Machine Learning\n",
    "\n",
    "Página pessoal: [smastelini](https://smastelini.github.io/)</br>\n",
    "e-mail: saulomastelini@gmail.com\n",
    "\n",
    "---\n",
    "\n",
    "## Sumário:\n",
    "\n",
    "- Naive Bayes\n",
    "- Validação cruzada\n",
    "- Processamento de texto básico\n",
    "- Visualização de fronteiras de decisão\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relembrar é viver\n",
    "\n",
    "- Supervisionado vs. não-supervisionado\n",
    "- Classificação vs. regressão\n",
    "- Pipeline: aquisição, pré-processamento, modelagem, avaliação\n",
    "- k-NN: classificador e regressor baseado em distâncias:\n",
    "    - Distância euclidiana, manhattan, minkowski, cosseno\n",
    "- Holdout: particionamento dos dados entre treino e teste\n",
    "- Avaliação de modelos de classificação\n",
    "\n",
    "<img src=\"img/a2/matriz-confusao.png\" width=\"500\">\n",
    "    <figcaption>Matriz de Confusão: contagem de erros e acertos do classificador por tipo</figcaption>\n",
    "</img>\n",
    "\n",
    "- Legenda:\n",
    "    - TP: Verdadeiro positivo\n",
    "    - FP: Falso positivo\n",
    "    - TN: Verdadeiro negativo\n",
    "    - FN: Falso negativo\n",
    "- Linhas e colunas para cada classe\n",
    "- Agregação `macro` para problemas multiclasse\n",
    "    - Binariza o problema: um vs. todos\n",
    "    - Uma métrica por classe\n",
    "    - Calcula a média no final\n",
    "    - Da peso igual para todas as classes\n",
    "- Agregação `micro` para problemas multiclasse\n",
    "    - Calcula o total de de todas as classes para gerar a métrica\n",
    "    - Dá peso igual por instância\n",
    "    - Útil para problemas desbalanceados\n",
    "\n",
    "\n",
    "**Populares métricas derivadas da matriz de confusão:**\n",
    "\n",
    "1. Acurácia: mede o desempenho geral\n",
    "    $$Accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$$\n",
    "2. Precisão (P): testa se o modelo é capaz de identificar a classe positiva\n",
    "    $$\\text{Precision} = \\dfrac{TP}{FP + TP}$$\n",
    "3. Recall ou Revocação (R): testa se o modelo é capaz de distribuir os esforços entre as classes\n",
    "    $$\\text{Recall} = \\dfrac{TP}{TP + FN}$$\n",
    "4. F1-score\n",
    "    $$F1 = \\dfrac{2 \\times P \\times R}{P + R}$$\n",
    "\n",
    "**Exemplo macro vs micro:**\n",
    "\n",
    "Suponha que tenhamos o seguinte exemplo:\n",
    "\n",
    "<img src=\"img/a3/ex-matriz-conf.png\" width=\"400\">\n",
    "    <figcaption>Ex: cálculo com matriz de confusão</figcaption>\n",
    "</img>\n",
    "\n",
    "- Classe A: 1 TP and 1 FP\n",
    "- Classe B: 10 TP and 90 FP\n",
    "- Classe C: 1 TP and 1 FP\n",
    "- Classe D: 1 TP and 1 FP\n",
    "\n",
    "- $P_A = P_C = P_D = 0.5$\n",
    "- $P_B = 0.1$\n",
    "\n",
    "Calculando os resultados finais:\n",
    "\n",
    "- Macro: $P = \\dfrac{0.5 + 0.1 + 0.5 + 0.5}{4} = 0.4$\n",
    "- Micro: $P = \\dfrac{1 + 10 + 1 + 1}{2 + 100 + 2 + 2} = \\dfrac{13}{106} \\approx 0.123$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Importacões básicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # para fazer os gráficos\n",
    "from sklearn.datasets import make_moons  # gerar os dados sintéticos\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para visualizar fronteiras de decisão (mais tarde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptado de: https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Utilities/ML-Python-utils.py\n",
    "def plot_decision_boundaries(X, y, model_class, **model_params):\n",
    "    \"\"\"Function to plot the decision boundaries of a classification model.\n",
    "\n",
    "    This uses just the first two columns of the data for fitting \n",
    "    the model as we need to find the predicted value for every point in \n",
    "    scatter plot.\n",
    "    Arguments:\n",
    "            X: Feature data as a NumPy-type array.\n",
    "            y: Label data as a NumPy-type array.\n",
    "            model_class: A Scikit-learn ML estimator class \n",
    "            e.g. GaussianNB (imported from sklearn.naive_bayes) or\n",
    "            LogisticRegression (imported from sklearn.linear_model)\n",
    "            **model_params: Model parameters to be passed on to the ML estimator\n",
    "    \n",
    "    Typical code example:\n",
    "            plt.figure()\n",
    "            plt.title(\"KNN decision boundary with neighbros: 5\",fontsize=16)\n",
    "            plot_decision_boundaries(X_train,y_train,KNeighborsClassifier,n_neighbors=5)\n",
    "            plt.show()\n",
    "    \"\"\"\n",
    "    from matplotlib import colormaps\n",
    "    import matplotlib.markers as mmarkers\n",
    "    def mscatter(x, y, ax=None, m=None, **kw):\n",
    "        if not ax: ax=plt.gca()\n",
    "        sc = ax.scatter(x,y,**kw)\n",
    "        if (m is not None) and (len(m)==len(x)):\n",
    "            paths = []\n",
    "            for marker in m:\n",
    "                if isinstance(marker, mmarkers.MarkerStyle):\n",
    "                    marker_obj = marker\n",
    "                else:\n",
    "                    marker_obj = mmarkers.MarkerStyle(marker)\n",
    "                path = marker_obj.get_path().transformed(\n",
    "                            marker_obj.get_transform())\n",
    "                paths.append(path)\n",
    "            sc.set_paths(paths)\n",
    "        return sc\n",
    "\n",
    "    try:\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).flatten()\n",
    "    except:\n",
    "        print(\"Coercing input data to NumPy arrays failed\")\n",
    "\n",
    "    # Reduces to the first two columns of data\n",
    "    reduced_data = X[:, :2]\n",
    "    # Instantiate the model object\n",
    "    model = model_class(**model_params)\n",
    "    # Fits the model with the reduced data\n",
    "    model.fit(reduced_data, y)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    \n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    # Meshgrid creation\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh using the model.\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    # Predictions to obtain the classification results\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    # Right now it only works with binary tasks: expand both dictionaries or use cmap for multiclass tasks\n",
    "\n",
    "    cmap = colormaps['brg']\n",
    "\n",
    "    markers = {0: \"o\", 1: \"^\", 2: \"s\", 3: \"D\", 4: \"+\", 5: \"*\"}\n",
    "\n",
    "    m = list(map(lambda i: markers[i], y))\n",
    "    \n",
    "    # Plotting\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)\n",
    "    mscatter(x=X[:, 0], y=X[:, 1], c=y, m=m, cmap=cmap, alpha=1)\n",
    "    \n",
    "    plt.xlabel(\"x1\",fontsize=15)\n",
    "    plt.ylabel(\"x2\",fontsize=15)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivação e Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pouco de notação:\n",
    "\n",
    "- **Eventos:** algum fenômeno de interesse.\n",
    "    - Por exemplo: ônibus atrasar, ganhar uma rifa, ser aprovado em uma disciplina.\n",
    "- **Evidências:** observações e subsídios que temos para respaldar a probabilidade de algo (um evento) acontencer.\n",
    "    - Por exemplo: qual é a probabilidade do ônibus atrasar (evento), dado que está chovendo (evidência).\n",
    "- Eventos e evidências representados por letras em caixa alta: A, B, etc.\n",
    "- Valores específicos de eventos representados por letras ou palavras em minúsculo.\n",
    "- O operador \"|\" é utilizado para denotar a expressão \"dado que\". \n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "\"Qual é a probabilidade de um animal ser um cachorro, dado que o barulho que ele faz é 'miau'?\"\n",
    "\n",
    "- Ser um animal: A\n",
    "- Barulho que faz: B\n",
    "\n",
    "$\\rightarrow P(A=\\text{cachorro} | B=\\text{miau})$\n",
    "\n",
    "- Os eventos são categorias, ou em nosso caso, as classes do problema.\n",
    "- As evidências podem de tipos variados: contagens, categorias, valores numéricos, etc.\n",
    "\n",
    "- O teorema de Bayes é dado pela seguinte expressão.\n",
    "\n",
    "$$P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "- $P(A|B)$ é chamado de *posterior*, ou seja, a probabilidade de um evento, dadas as evidências.\n",
    "- $P(B|A)$ é chamado de *likelihood*. Representa a probalidade da evidência acontecer, dado que o evento também ocorreu. \n",
    "- $P(A)$ é chamado de *prior* e representa o nosso conhecimento prévio acerca dos eventos. Ex: qual é probabilidade do ônibus atrasar, sem que tenhamos quaisquer informações extras?\n",
    "- $P(B)$ é chamado de *evidence*.\n",
    "\n",
    "Com essas informações podemos pensar na equação do teorema de uma forma diferente:\n",
    "\n",
    "$$\\text{posterior} = \\dfrac{\\text{likelihood}\\times\\text{prior}}{\\text{evidence}}$$\n",
    "\n",
    "### 1.1. Um pouco mais de intuição\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(random_state=42, noise=0.05)\n",
    "\n",
    "markers = [\"^\", \"s\"]\n",
    "for c in np.unique(y):\n",
    "    mask = y == c\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], marker=markers[c])\n",
    "\n",
    "plt.scatter(1.1, 0.4, marker=\"x\", c=\"green\", s=100)\n",
    "plt.scatter(0.0, 0.73, marker=\"x\", c=\"red\", s=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir da visualização dos dados e da nossa intuição nós conseguimos pensar em termos de probabilidade: \"qual é a maior chance dos pontos demarcados com X serem de uma classe ou outra?\"\n",
    "\n",
    "Implicitamente estamos fazendo isso:\n",
    "\n",
    "- qual é a chance de X ser triângulo?\n",
    "- qual é a chance de X ser quadrado?\n",
    "\n",
    "Nossa resposta é a classe que gerar a maior probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Um problema real\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/a3/tipos-vinho.jpg\" width=\"500px\">\n",
    "  <figcaption>Tipos de vinho. Fonte: <a href=\"https://mastervinho.com.br/tipos-de-vinho/\">Master Vinho</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um problema de classificação de tipos de vinho\n",
    "dataset = load_wine()\n",
    "\n",
    "# Separa os atributos preditivos e o target (classes)\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "print(f\"Número de exemplos: {len(X)}\")\n",
    "\n",
    "print(\"Atributos descritivos:\")\n",
    "for att in dataset.feature_names:\n",
    "    print(f\"-> {att}\")\n",
    "\n",
    "print(\"\\n Tipos de vinho:\")\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora? Não dá para fazer um gráfico para nos ajudar. Como lidamos com isso?\n",
    "\n",
    "...\n",
    "\n",
    "Hora de retomar o teorema de Bayes e definir o classificador Naive Bayes.\n",
    "\n",
    "Por que esse algoritmo leva esse nome? \n",
    "\n",
    "> Naive = ingênuo\n",
    "\n",
    "### 1.3. Rapadura é doce, mas não é mole\n",
    "\n",
    "\n",
    "Na vida real, as evidências normalmente não são independentes. Essas grandezas acontecem de forma conjunta.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "> Evento (alvo da classificação): viagem atrasar ou não.\n",
    "\n",
    "- uma viagem pode atrasar por conta de um pneu furado em um buraco do asfalto\n",
    "\n",
    "mas\n",
    "\n",
    "\n",
    "- a chance do motorista não ver o buraco é maior em um dia chuvoso (visibilidade reduzida)\n",
    "- a velocidade média também (deveria) ser menor no caso de pista molhada\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://media.istockphoto.com/vectors/office-worker-in-car-vector-id1178291082?k=20&m=1178291082&s=612x612&w=0&h=iIzb4A_uCKbzHLhBjxCXHZaZnavOofbJAP63UJTYMhM=\" width=\"60%\">\n",
    "  <figcaption><a href=\"https://www.istockphoto.com/br/ilustra%C3%A7%C3%B5es/man-driving-rain\">Fonte</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "- Se o nosso evento de interesse é \"atraso na viagem\":\n",
    "  - As evidências (atributos ou *features*) não são independentes.\n",
    "\n",
    "- Nesse exemplo, as evidências mencionadas seriam:\n",
    "  - pneu furou ou não\n",
    "  - condição climática\n",
    "  - visibilidade da pista\n",
    "  - estado de conservação da pista\n",
    "  - e por aí vai...\n",
    "\n",
    "- Voltando ao teorema de Bayes:\n",
    "\n",
    "$$P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "- O que gostaríamos de responder é:\n",
    "  - $P(\\text{Atraso} = \\text{sim} | \\text{Pneu Furado}, \\text{Condição Climática}, ..., )$\n",
    "  - $P(\\text{Atraso} = \\text{não} | \\text{Pneu Furado}, \\text{Condição Climática}, ..., )$\n",
    "- Observação:\n",
    "    - É comum utilizar os valores das observações para evitar usar o \"=\" toda vez.\n",
    "  - Exemplo: \"probabilidade de atrasado, dado que o pneu furou e choveu\" $\\rightarrow P(\\text{atrasado} | \\text{pneu-furou}, \\text{choveu})$\n",
    "\n",
    "- Problema: não sabemos responder $P(B|\\text{Atraso} = \\text{sim})$ e $P(B|\\text{Atraso} = \\text{não})$\n",
    "  - As nossas evidências dependem umas das outras.\n",
    "\n",
    "1. Uma coisa é perguntar (fatos isolados):\n",
    "  - Qual a chance de um pneu ter furado se a viagem atrasou (ou não)?\n",
    "  - Qual é probabilidade de estar chovendo (durante o percurso) se a viagem atrasou (ou não)?\n",
    "  - ...\n",
    "2. Agora, perguntar (fenômenos simultânemos):\n",
    "  - Qual é a chance de um pneu ter furado **e** estar chovendo **e** a visibilidade da pista estar ruim **e** a pista estar mal conservada (...) se a viagem atrasou (ou não)?\n",
    "\n",
    "- A segunda opção parece mais difícil porque realmente é! O nome disso é **probabilidade conjunta** e não é algo fácil de ser calculado, normalmente.\n",
    "\n",
    "\n",
    "### 1.3. Aplicando o teorema de Bayes concretamente\n",
    "\n",
    "- Vamos mudar a notação do teorema de Bayes para ficarmos mais próximos de AM.\n",
    "\n",
    "$$P(c_k | \\mathbf{x}) = \\dfrac{P(\\mathbf{x}|c_k)\\times P(c_k)}{P(\\mathbf{x})}$$\n",
    "\n",
    "- $c_k$ representa uma das classes do nosso problema.\n",
    "  - Ex: Os tipos de vinho no problema anterior, ou se haverá atraso na viagem no exemplo hipotético.\n",
    "- $\\mathbf{x}$ representa as *features* do nosso problema (um vetor/*array*).\n",
    "  - Cada posição pode ser acessada: $x_i$, assumindo que $i$ varia entre $[0, m - 1]$ e $m$ é o número de *features*.\n",
    "\n",
    "- Vamos simplificar algo aqui:\n",
    "  - O denominador será sempre o mesmo, independente da classe.\n",
    "  - Para aplicar AM nós só queremos saber qual classe tem mais chance de ser a verdadeira. Certo?\n",
    "  - Então ignoraremos o denominador e vamos assumir que ele é um valor constante comum à todas as classes:\n",
    "\n",
    "$$P(c_k | \\mathbf{x}) = \\dfrac{P(\\mathbf{x}|c_k)\\times P(c_k)}{P(\\mathbf{x})} \\propto P(\\mathbf{x}|c_k)\\times P(c_k)$$\n",
    "\n",
    "- A solução para aplicar o teorema de Bayes em aprendizado de máquina é simplificar o problema.\n",
    "  - Partimos de uma hipótese bem ingênua:\n",
    "\n",
    "> Vamos assumir que todas as nossas evidências (*features*) são independentes umas das outras!\n",
    "\n",
    "- Suposição ousada. Assumir isso significa que para o algoritmo que vamos criar, por exemplo, o fato de estar chovendo não nenhuma relação com a visibilidade da pista. Pode parecer algo meio idiota de se pensar e, de fato, de certa forma é (dado nosso exemplo) :P\n",
    "- Matematicamente falando:\n",
    "\n",
    "$$P(x_i | x_0, x_1, ..., x_{i-1}, x_{i + 1}, ..., x_{m - 1}, c_k) = P(x_i | c_k).$$\n",
    "\n",
    "- Em muitos problemas reais essa simplificação dá conta do recado muito bem!\n",
    "- **Se as features são independentes**, podemos re-escrever a versão simplificada do teorema de Bayes como:\n",
    "\n",
    "$$P(c_k | \\mathbf{x}) = P(c_k)\\prod_{i=0}^{m-1} P(x_i|c_k)$$\n",
    "\n",
    "#### 1.3.1. Detalhe extra: a transformação log\n",
    "\n",
    "- Multiplicar vários números (normalmente pequenos) tende a acarretar problemas aproximação numérica.\n",
    "- Vamos utilizar um \"macete\" matemático para \"escapar\" das multiplicações:\n",
    "  - Aplicar a operação $\\log$ em tudo.\n",
    "- Propriedade: $\\log{(a \\times b)} = \\log{a} + \\log{b}$\n",
    "- Então:\n",
    "\n",
    "$$\\log(P(c_k | \\mathbf{x})) = \\log(P(c_k)) + \\sum_{i=0}^{m-1} \\log(P(x_i|c_k))$$\n",
    "\n",
    "- Para recuperar o valor original, basta aplicar a operação de exponenciação: $v = e^{\\log{v}}$.\n",
    "- Existem várias variações do algoritmo Naive Bayes (NB).\n",
    "  - Variante voltada para dados numéricos: Gaussian Naive Bayes.\n",
    "\n",
    "### 1.4. Aproximação com distribuição Gaussiana\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://pbs.twimg.com/media/EHBVhwDWsAEXeaH.jpg\" width=\"40%\">\n",
    "  <figcaption><a href=\"https://twitter.com/ai_memes/status/1184540745946017792\">Fonte</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Aproxima os valores de $P(x_i | c_k)$ através de distribuições gaussianas (elas tem propriedades interessantes e modelam muitos fenômenos da vida real). Podemos calcular a probabilidade (função densidade) de um dado valor $x$ em uma distribuição gaussiana através de:\n",
    "\n",
    "$$f(x) = \\dfrac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $\\mu$: média\n",
    "- $\\sigma$: desvio padrão\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1200px-Normal_Distribution_PDF.svg.png\" width=\"50%\">\n",
    "    <figcaption>Fonte: <a href=\"https://pt.wikipedia.org/wiki/Distribui%C3%A7%C3%A3o_normal\">Wikipedia</a></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return (\n",
    "        (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(\n",
    "            - ((x - mu) ** 2) / (2 * (sigma ** 2))\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5), ncols=3)\n",
    "\n",
    "# Escolhi umas das features arbitrariamente\n",
    "x_classe_0 = X[y == 0, 4]\n",
    "x_classe_1 = X[y == 1, 4]\n",
    "x_classe_2 = X[y == 2, 4]\n",
    "\n",
    "# Gera histogramas para cada classe\n",
    "ax[0].hist(x_classe_0, density=True)\n",
    "ax[1].hist(x_classe_1, density=True)\n",
    "ax[2].hist(x_classe_2, density=True)\n",
    "\n",
    "# Define 100 números igualmente espaçados, entre o min e o max\n",
    "xs_0 = np.linspace(min(x_classe_0), max(x_classe_0), 100)\n",
    "xs_1 = np.linspace(min(x_classe_1), max(x_classe_1), 100)\n",
    "xs_2 = np.linspace(min(x_classe_2), max(x_classe_2), 100)\n",
    "\n",
    "# Exibe os valores das distribuições gaussianas\n",
    "ax[0].plot(xs_0, gaussian_pdf(xs_0, np.mean(x_classe_0), np.std(x_classe_0, ddof=1)))\n",
    "ax[1].plot(xs_1, gaussian_pdf(xs_1, np.mean(x_classe_1), np.std(x_classe_1, ddof=1)))\n",
    "ax[2].plot(xs_2, gaussian_pdf(xs_2, np.mean(x_classe_2), np.std(x_classe_2, ddof=1)))\n",
    "\n",
    "ax[0].set_title(\"Classe 0\")\n",
    "ax[1].set_title(\"Classe 1\")\n",
    "ax[2].set_title(\"Classe 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplos aleatórios dados em aula para ilustrar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {\n",
    "    1: 2,\n",
    "    \"batata\": \"frita\",\n",
    "    \"True\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"batata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(X.shape[1]):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        # Seleciona as classes do problema\n",
    "        self.classes = np.sort(np.unique(y))\n",
    "        \n",
    "        # Já vou salvar o log do nosso conhecimento a priori\n",
    "        self.logprior = {}\n",
    "        # Aqui é que entram as gaussianas\n",
    "        self.likelihood = {}\n",
    "        # Para cada classe\n",
    "        for c in self.classes:\n",
    "            self.likelihood[c] = {}\n",
    "\n",
    "            class_mask = y == c\n",
    "            \n",
    "            # Prior\n",
    "            self.logprior[c] = np.log(len(y[class_mask]) / len(y))\n",
    "            \n",
    "            Mu = np.mean(X[class_mask], axis=0)\n",
    "            Sigma = np.std(X[class_mask], ddof=1, axis=0)\n",
    "            \n",
    "            # Para cada atributo preditivo\n",
    "            for j in range(X.shape[1]):\n",
    "                # Guardamos uma função gaussiana do likelihood\n",
    "                self.likelihood[c][j] = (\n",
    "                    # Função de densidade de probabilidade da distribuição\n",
    "                    # Gaussiana\n",
    "                    lambda x, mu=Mu[j], sigma=Sigma[j] : (\n",
    "                        (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(\n",
    "                            - ((x - mu) ** 2) / (2 * (sigma ** 2))\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Numero de amostras x numero de classes\n",
    "        proba = np.zeros((len(X), len(self.classes)))\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # Adicionamos a probabilidade a priori (estamos utilizando a propriedade aditiva dos logs)\n",
    "            proba[:, c] += self.logprior[c]\n",
    "            for j in range(X.shape[1]):\n",
    "                # Usaremos o log do likelihood\n",
    "                p = self.likelihood[c][j](X[:, j])\n",
    "                # Valores muito proximos de zero podem nos dar problema\n",
    "                # Melhor evitá-los (considerar como zero, logo de uma vez)\n",
    "                mask = p > 0\n",
    "                # Usa a máscara binária criada para filtrar os valores\n",
    "                proba[mask, c] += np.log(p[mask])\n",
    "        \n",
    "        # Aplicamos a funçao exponencial para desfazer o efeito do log\n",
    "        proba = np.exp(proba)\n",
    "\n",
    "        # Lembre-se que nós removemos o P(x) do denominador da equação de Bayes\n",
    "        # para simplificar os calculos. Precisamos normalizar os valores para\n",
    "        # obter probabilidades\n",
    "        return proba / np.sum(proba, axis=1)[:, None]\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Retorna a classe mais provavel\n",
    "        return self.classes[np.argmax(self.predict_proba(X), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(X[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hora de avaliar a nossa implementação.\n",
    "\n",
    "## 2. Validação cruzada: indo além do Holdout\n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/a3/cv.png\" width=\"600px\">\n",
    "    <figcaption>Fonte: sklearn</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=8)\n",
    "\n",
    "\n",
    "for train, test in cv.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    nb = GaussianNaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    accs.append(\n",
    "        accuracy_score(y_test, nb.predict(X_test))\n",
    "    )\n",
    "\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(accs) / len(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "results = cross_validate(\n",
    "    estimator=GaussianNB(),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n",
    "    cv=KFold(n_splits=10, shuffle=True, random_state=8)\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizando as fronteiras de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split  # Holdout\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"k-NN\")\n",
    "plot_decision_boundaries(X_train, y_train, KNeighborsClassifier, n_neighbors=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gaussian NB\")\n",
    "plot_decision_boundaries(X_train, y_train, GaussianNB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exemplo com processamento de texto\n",
    "\n",
    "- Utilizaremos um dataset de texto\n",
    "- Opinião sobre livros na Amazon Brasil:\n",
    "    - Fonte dos dados: [Larissa Britto and Luciano Pacífico](https://github.com/larifeliciana/books-reviews-portuguese).\n",
    "- Estamos lidando com dados não-estruturados/não-tabulares\n",
    "    - Precisamos \"estruturar\" esses dados\n",
    "- Alguns passos usuais em processamento de texto:\n",
    "    - _Tokenization_: separar os componentes do texto\n",
    "        - No nosso exemplo, palavra por palavra\n",
    "        - Ignorar pontuação e caracteres especiais\n",
    "    - Remover _stop-words_: remoção de palavras que não tem função semântica alta no texto\n",
    "    - _Lemmatization_: utilizar o lema de uma palavra, para reduzir a dimensão e melhor capturar a semântica.\n",
    "        - Ex 1.: tiver, tinha, tenho, tem $\\rightarrow$ ter\n",
    "    - Remoção de acentos\n",
    "    - Conversão de Maiúsculas para minúsculas\n",
    "    - Entre outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/a3/books_pt_neg.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    negative = f.readlines()\n",
    "\n",
    "with open(\"data/a3/books_pt_pos.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    positive = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(negative), len(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "all_text.extend(positive)\n",
    "all_text.extend(negative)\n",
    "\n",
    "all_labels = [1] * len(positive)\n",
    "all_labels.extend([-1] * len(negative))\n",
    "\n",
    "rng = random.Random(8)\n",
    "\n",
    "for i in range(len(all_text) - 1, 0, -1):\n",
    "    j = rng.randint(0, i + 1)\n",
    "\n",
    "    # Troca de posição\n",
    "    all_text[i], all_text[j] = all_text[j], all_text[i]\n",
    "    all_labels[i], all_labels[j] = all_labels[j], all_labels[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Texto: Não-estruturado $\\rightarrow$ estruturado\n",
    "\n",
    "- _Bag-of-words_ (BOW)\n",
    "    - Contagem de cada termo em cada texto, considerando todo o _corpus_ (todos os documentos)\n",
    "    - Ordenar as palavras. A posição é o id\n",
    "- Essa representação tem alguns problemas\n",
    "    - Peso das Stop-words\n",
    "    - Documentos mais longos terão maiores valores de contagem das palavras\n",
    "- Aplicaremos um passo de normalização dos dados obtidos via BOW: [TF-IDF](https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275) (_Term Frequency times Inverse Document Frequency_)\n",
    "    - Divide a contagem de uma palavra em um documento pela soma das contagens naquele documento (TF)\n",
    "        - Obtemos proporções aqui\n",
    "    - Penalizamos palavras que aparecem muito no corpus para atenuar seu impacto nos modelos\n",
    "        - Por exemplo: o, os, a, as, da, de, do, etc.\n",
    "    \n",
    "- Matematicamente falando:\n",
    "    - Assuma que:\n",
    "        - w: uma palavra\n",
    "        - d: um documento, contendo várias palavras\n",
    "        - D: o conjunto de todos os documentos, ou seja, o _corpus_\n",
    "        - N: o número de documentos no _corpus_\n",
    "    - $\\text{TF}(w, d) = \\log(1 + f(w, d))$\n",
    "    - $\\text{IDF}(w, D) = \\log\\left(\\dfrac{N}{f(w, D)}\\right)$\n",
    "    - $\\text{TF-IDF}(w, d, D) = TF(w, d) * IDF(w, D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Multinomial Naive Bayes\n",
    "\n",
    "- Nesse exemplo, utilizaremos o Multinomial Naive Bayes:\n",
    "    - Considera a distribuição de cada classe $y$ através de um vetor $\\theta_y = (\\theta_{y0}, \\theta_{y1}, ..., \\theta_{yn-1})$\n",
    "    - $n$ é número de features, ou seja, o tamanho do vocabulário\n",
    "    - $\\theta_{yi}$ é a probabilidade $P(x_i | y)$, da feature $x_i$ aparecer em uma amostra com classe $y$\n",
    "\n",
    "- Fórmula para aproximar $\\theta$:\n",
    "\n",
    "$$\\hat{\\theta}_{yi} = \\dfrac{N_{yi} + \\alpha}{N_y + \\alpha n}$$\n",
    "\n",
    "- Na expressão acima:\n",
    "    - $N_{yi} = \\sum_{x \\in \\text{Treino}}$: número de vezes (ou proporção) que o $i$-ésimo atributo aparece em amostras da classe $y$.\n",
    "    - $N_y = \\sum_{i=1}^n N_{yi}$: soma total das contagens (ou proporções).\n",
    "    - $\\alpha \\ge 0$: é um parâmetro de suavização para evitar erros numéricos e atenuar o impacto de features não-presentes no treino. \n",
    "\n",
    "\n",
    "- Essa versão do NB utiliza as frequências dos dados (ou proporções) como entrada, ao invés de utilização uma distribuição de dados\n",
    "- É um versão ainda mais simples do que o Gaussian NB\n",
    "- Baseline forte em processamento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos à modelagem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    MultinomialNB()\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino\n",
    "\n",
    "```py\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.transform(textos)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X, y)\n",
    "```\n",
    "\n",
    "# Teste\n",
    "\n",
    "```py\n",
    "X_novos = tfidf.transform(novos_textos)\n",
    "nb.predict(X_novos)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results = cross_validate(\n",
    "    estimator=model,\n",
    "    X=all_text,\n",
    "    y=all_labels,\n",
    "    scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n",
    "    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame.from_dict(text_results)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "final_model.fit(all_text, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.predict(\n",
    "    [\n",
    "        \"Achei esse livro uma porcaria. A história é muito rasa e previsível! Não recomendo a ninguém!\",\n",
    "        \"Que viagem incrível! Me transportei para os meus tempos de infância. Não consegui desgrudar os olhos do livro enquanto não terminei!!\",\n",
    "        \"Livro horrível, eheheh. Até esqueci de comer e beber por horas e nem me dei conta de tanta concentração! Acabou com minha vida social!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_X = final_model.steps[0][1].transform([\"Será que funciona?\"])\n",
    "t_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_mask = t_X > 0\n",
    "t_X[t_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.steps[0][1].get_feature_names_out()[t_mask.toarray()[0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa\n",
    "\n",
    "1. Refaça o pipeline anterior utilizando o algoritmo k-NN visto nas aulas anteriores. Você é encorajado a modificar os parâmetros do k-NN buscando melhorar os resultados preditivos.\n",
    "\n",
    "_Dica:_ nesse caso, o TF-IDF age como um passo de normalização dos dados. Não precisa usar o Standard Scaler nesse exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta tarefa 1 aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Comente os resultados obtidos. Qual uma possível explicação para as diferenças de desempenho, tendo em vista as fronteiras de decisão vistas anteriormente, tanto para o k-NN quanto para o NB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b27b2a9f272874e098660428b28f5afa65c7850d82ff592660d49a141d883cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
